{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Similarity search methods\n",
    "\n",
    "1. Jaccard = intersection(A,B) / union(A,B). For example, A = {1,2,3} and B = {2,4}, our resulkting Jaccard = 0.25\n",
    "2. Levenshtein (similar to Jaccard but requires preprocessign for n-gram representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(sentc_one: str, sentc_two: str):\n",
    "    # Convert to sets\n",
    "    sentc_one = set(sentc_one.split())\n",
    "    sentc_two = set(sentc_two.split())\n",
    "    # Calculate\n",
    "    shared = sentc_one.intersection(sentc_two)\n",
    "    union = sentc_one.union(sentc_two)\n",
    "    return len(shared) / len(union)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('Paul is cool', 'Cool person forever Paul is coolest is coolest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse and Dense Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TF-IDF (Sparse)\n",
    "\n",
    "Term Frequency = f(q, D) / f(t, D)\n",
    "\n",
    "f(q, D) - Frequency of query q in Document D\n",
    "f(t, D) - Frequency of all terms in Document D (total num terms)\n",
    "\n",
    "Inverse Document Frequency or IDF = log (N / N(q='forest'))\n",
    "It is calculate for each document\n",
    "\n",
    "N is the total number of documents\n",
    "N(q=\"forest\") is number of documents containing the query forest\n",
    "\n",
    "2. BM25 (Sparse)\n",
    "3. SBERT (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"purple is the best city in the forest\".split()\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
    "c = \"it is not often you find soggy bananas on the street\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we'll merge all docs into a list of lists for easier calculation of IDF\n",
    "docs = [a , b, c]\n",
    "# print(docs)\n",
    "\n",
    "def tfidf(word, sentence):\n",
    "    # term frequency\n",
    "    freq = sentence.count(word)\n",
    "    tf = freq / len(sentence)\n",
    "    \n",
    "    # inverse document frequency\n",
    "    idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "\n",
    "    # tf-idf\n",
    "    tf_idf = round(tf*idf, 4)\n",
    "\n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0596"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('forest', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an',\n",
       " 'and',\n",
       " 'art',\n",
       " 'bananas',\n",
       " 'best',\n",
       " 'city',\n",
       " 'find',\n",
       " 'forest',\n",
       " 'getting',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'not',\n",
       " 'often',\n",
       " 'on',\n",
       " 'purple',\n",
       " 'soggy',\n",
       " 'street',\n",
       " 'the',\n",
       " 'there',\n",
       " 'throwing',\n",
       " 'to',\n",
       " 'way',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Document vectors\n",
    "vocab = set(a+b+c)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror our vocab as vector, for each document\n",
    "\n",
    "vec_a = []\n",
    "vec_b = []\n",
    "vec_c = []\n",
    "\n",
    "for word in vocab:\n",
    "    vec_a.append(tfidf(word, a))\n",
    "    vec_b.append(tfidf(word, b))\n",
    "    vec_c.append(tfidf(word, c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0265,\n",
       " 0.0265,\n",
       " 0.053,\n",
       " 0.0098,\n",
       " 0.0,\n",
       " 0.0265,\n",
       " 0.0,\n",
       " 0.0265,\n",
       " 0.0,\n",
       " 0.0265,\n",
       " 0.0098,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0098,\n",
       " 0.0,\n",
       " 0.0098,\n",
       " 0.0,\n",
       " 0.0098,\n",
       " 0.0265,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0265,\n",
       " 0.0265]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25\n",
    "BM25 is an optimized version of TFIDF\n",
    "\n",
    "One of the problems with the TFIDF method is that as the frequency of queries found in a document increase, the score increases linearly.\n",
    "\n",
    "Imagine 1000 word article with word dog appearing 10 times = good chance article is talking about dog\n",
    "\n",
    "Now if you double words that mention dog to 20, TFIDF will double as well. \n",
    "\n",
    "Logically that shouldnt be the case. That means that the original document with dog 10times is a document that is half as relevant to dogs as the article with dog 20 times. BM25 tries to normalize this exaggeration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"purple is the best city in the forest\".split()\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
    "c = \"it is not often you find soggy bananas on the street\".split()\n",
    "d = \"green should have smelled more tranquil but somehow it just tasted rotten\".split()\n",
    "e = \"joyce enjoyed eating pancakes with ketchup\".split()\n",
    "f = \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs = [a, b, c, d, e, f]\n",
    "N = len(docs) # Number of documents\n",
    "\n",
    "# Recall tfidf\n",
    "def tfidf(word, sentence):\n",
    "    # term frequency\n",
    "    freq = sentence.count(word)\n",
    "    tf = freq / len(sentence)\n",
    "    \n",
    "    # inverse document frequency\n",
    "    N_q = sum([1 for doc in docs if word in doc]) # Number of docs that contain the word  \n",
    "    idf = np.log10(N / N_q)\n",
    "\n",
    "    # tf-idf\n",
    "    tf_idf = round(tf*idf, 4)\n",
    "\n",
    "    return tf_idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization new features\n",
    "# 1. Average Document length (Sum of All len(sent) across docs / N)\n",
    "avgdl = sum(len(sentence) for sentence in docs) / N \n",
    "\n",
    "# 2. k and b - Adjustable parameters that control ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs) # Number of documents\n",
    "#bm25 implementation\n",
    "def bm25(word, sentence, k=1.2, b=0.75):\n",
    "    # term frequency\n",
    "    freq = sentence.count(word) \n",
    "    \n",
    "    tf = (freq * (k + 1)) / (freq + k * (1 - b + b * len(sentence) / avgdl))\n",
    "\n",
    "    # inverse document frequency\n",
    "    N_q = sum([1 for doc in docs if word in doc]) # Number of docs that contain the word    \n",
    "    idf = np.log(((N - N_q + 0.5) / (N_q + 0.5)) + 1)\n",
    "\n",
    "    return round(tf*idf, 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7677"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25('purple', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing TF-IDF and BM25 algorithms\n",
    "The TF-IDF score increases linearly with the frequency for the matching term \n",
    "\n",
    "The BM25 algorithm increases logarithmically.\n",
    "\n",
    "In most cases BM25 is more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vectors with SBERT\n",
    "The effect of using dense vectors is that it lets you represent words in a more meaningful manner\n",
    "\n",
    "For example words like hi == hello in their vector similarity\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea2f73b6b3337bbcd990992f986e589ee5babb21456e0839dadedb1841132886"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
